- 反向传播算法
    - 符号表示
        - L
            - total no. of layers in network
        - `$s_l$`
            - no. of units (not counting bias unit) in layer l
            - 输出层的数目可写为`$s_L$`或者K
    - K元分类代价函数
        ```math
        J(\Theta)=-\frac{1}{m}\left[\sum_{i=1}^{m}\sum_{k=1}^{K}y_k^{(i)}\log(h_\Theta(x^{(i)}))_k+(1-y_k^{(i)})\log(1-(h_\Theta(x^{(i)}))_k)\right]+\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(\Theta_{ji}^{(l)})^2
        ```
    - 梯度计算
        - `$\delta_j^{(l)}$` = "error" of node j in layer l
            ```math
            \delta_j^{(l)} = \frac{\partial cost}{\partial z_j^{(l)}}
            ```
        - For each output unit (layer L=4, m=1, lambda=0)
            ```math
            \delta^{(4)} = \frac{\mathrm{d}cost(i)}{\mathrm{d}z_1^{(4)}} = y - a^{(4)}
            
            \text{where}
                
            cost(i) = y^{(i)}\log h_\Theta(x^{(i)}) + (1-y^{(i)})\log (1-h_\Theta(x^{(i)}))
            
            \delta^{(l)} = \delta^{(l+1)}\frac{\partial z^{(l+1)}}{\partial z^{(l)}}
            
            \delta^{(3)} = (\Theta^{(3)})^T\delta^{(4)} .\!* g'(z^{(3)})
            
            \delta^{(2)} = (\Theta^{(2)})^T\delta^{(3)} .\!* g'(z^{(2)})
            
            \text{其中：}
            
            g'(z^{(3)}) = a^{(3)} .\!* (1 - a^{(3)})
            
            g'(z^{(2)}) = a^{(2)} .\!* (1 - a^{(2)})
            
            \text{偏导：}
            
            \frac{\partial}{\partial\Theta_{ij}^{(l)}}j(\Theta) = \frac{\partial cost}{\partial z_i^{(l+1)}} \frac{\partial z_i^{(l+1)}}{\partial\Theta_{ij}^{(l)}} =  \delta_i^{(l+1)}a_j^{(l)}
            ```
        - 对于整个训练集m
            - Set `$\Delta_{ij}^{(l)} = 0 \quad \text{(for all l, i, j)}$`
                - Used to compute `$\frac{\partial}{\partial\Theta_{ij}^{(l)}}j(\Theta)$`
            - For i = 1 to m
                - Set `$a^{(1)} = x^{(i)}$`
                - Perform forward propagation to conpute `$a^{(l)} \quad \text{for l = 2, 3, .., L}$`
                - Using `$y^{(i)}$`, compute `$\delta^{(L)} = a^{(L)} - y{(i)}$`
                    - 整体的代价函数开头为`$-1/m$`，有负号
                - Compute `$\delta^{(L-1)}, \delta^{(L-2)},..., \delta^{(2)}$`
                - `$\Delta_{ij}^{(l)} = \Delta_{ij}^{(l)} + a_j^{(l)}\delta_i^{(l+1)}$`
                     - Vectorized: `$\Delta^{(l)} = \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T$`
            - m次循环后
                ```math
                \begin{cases}
                D_{ij}^{(l)} = \dfrac{1}{m}\Delta_{ij}^{(l)},  & \text{if j = 0} \\
                \theta_j=\theta_j-\alpha\left[\dfrac{1}{m}\displaystyle\sum_{i=1}^{m}\left(h_\theta(x^{(i)})-y^{(i)}\right)x_j^{(i)}+\frac{\lambda}{m}\theta_j\right], & \text{if j = 1,2,3,...,n}
                \end{cases}
                ```
        - 理解反向传播
            - 多元复合函数求导法则
                - 考虑函数`$z = f(x, y)$`，其中`$x = g(t)$`，`$y = h(t)$`，`$g(t)$`和`$h(t)$`是可微函数，那么：
                    ```math
                    {\ dz \over dt}={\partial z \over \partial x}{dx \over dt}+{\partial z \over \partial y}{dy \over dt}
                    ```
            - `$\delta_j^{(l)}$` = "error" of cost for `$a_j^{(l)}$` (unit j in layer l)
                ```math
                \text{Formally,}
                
                \delta_j^{(l)} = \frac{\partial}{\partial z_j^{(l)}}cost(i)\quad \text{(for j >= 0)}
                
                \text{where}
                
                cost(i) = y^{(i)}\log h_\Theta(x^{(i)}) + (1-y^{(i)})\log (1-h_\Theta(x^{(i)}))
                ```
        - 梯度检测
            - 保证前向传播、后向传播的正确性
            - two-sided difference
                ```math
                \frac{\mathrm{d}}{\mathrm{d}\Theta}J(\Theta) \approx \frac{J(\Theta + \epsilon) - J(\Theta - \epsilon)}{2\epsilon}
                ```
                - 通常能得到比单侧差分更精确的结果
                - `$\epsilon$`可考虑取`$10^{-4}$`，数值太小会出错
                - 与反向传播的偏导数相比较以检验其正确性
                - 当验证了反向传播的正确性，就应当关闭梯度检测，因为其运行起来比反向传播慢很多
            - 向量化的`$\theta$`
                ```math
                \frac{\partial}{\partial\theta_1}J(\theta) \approx \frac{J(\theta_1 + \epsilon, \theta_2, \theta_3, ... , \theta_n) - J(\theta_1 - \epsilon, \theta_2, \theta_3, ... , \theta_n)}{2\epsilon}
                ```
        - 随机初始化
            - 在训练神经网络时将`$\Theta$`初始化为0起不到任何作用，因为不管多少次梯度下降后，每一层隐藏层中，各个单元的值都一样
            - 方法
                - 将`$\Theta^{(l)}_{ij}$`初始化为`$[-\epsilon, \epsilon]$`之间的随机值（此处的`$\epsilon$`与梯度检测中的无关）
                - `$np.random.uniform()$` 或者 `$\text{randint(0, 1)} * 2 \epsilon - \epsilon$` 
                - One effective strategy for choosing `$\epsilon$` is to base it on the number of units in the network. A good choice of `$\epsilon$` is `$\epsilon = \dfrac{\sqrt 6}{\sqrt {L_{in} + L_{out}}}$`, where `$L_{in} = s_l$` and `$L_{out} = s_{l+1}$` are the number of units in the layers adjacent to `$\Theta^{(l)}$`.
        - 网络架构
            - Reasonable default
                - 1 hidden layer, or if > 1 hidden layer, have same no. of hidden units in every layer (usually the more the better)
        - 代价函数非凸性问题
            - 神经网络中的代价函数`$J(\Theta)$`是非凸函数，理论上梯度下降算法和其他一些高级优化算法会收敛到局部最佳，实际操作中这通常不是一个大问题，虽然不能保证这些算法得到全局最优，但实际表现不错，通常能得到一个很好的局部最小值
